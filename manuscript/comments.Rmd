---
title: Post-publication comments on the deconvolution paper 
author: Aaron Lun
output:
  BiocStyle::html_document:
    fig_caption: false
    toc_float: true
---

# Dealing with zero counts

## Origin of zeroes 

Semi-systematic zeros are probably more relevant here than in bulk, due to the fact that you get pure cell types.
In bulk, you'd expect some contamination so the count would unlikely to be exactly zero in any particular sample.

## Dealing with dropouts 

Differences in dropout rates between cells should not affect scaling normalization.
This is because scaling normalization concerns itself with systematic (technical) differences in the **expected** expression between cells.
Whether such differences occur in the zero or non-zero components is irrelevant to the computed size factors. 
The distribution of expression only matters insofar as the robust average estimators are accurate.

# Inaccuracies with existing methods

## Applying existing methods on the highest abundances

The alternative approaches would be to use the arithmetic mean (for DESeq) or to remove a gene entirely if it contains any zeroes.
The former still results in biases, as a majority of zeroes in each cell means that cell-specific zeroes must be removed.
The latter leaves us with a set of high-abundance genes from which the size factor is computed.
This makes the non-DE assumption stronger; everything else being equal, it's easier to get a majority in a small set than a larger one.

One could argue that the high-abundance genes are more likely to be house-keeping genes.
However, that doesn't make them non-DE, especially when you throw in biological variability and coregulation driving systematic differences between cells.
Finally, it's not a sustainable strategy, as the set of non-zero genes will decrease with an increasing number of cells.
This will leave you with very few genes -- possibly in the double-digits -- which exacerbates the problem above.

Indeed, completely removing genes with any zero can directly bias the size factor estimates.
This is because you are more likely to select genes that are upregulated in cells with small size factors, in order to avoid zeroes.
Such upregulation can be genuine DE, or stochastic due to non-independent filtering (see below).
This results in overestimation of the size factors for such cells:

```{r}
set.seed(10000)
means <- cbind(matrix(1, nrow=1000, ncol=20), matrix(2, nrow=1000, ncol=20))
means[1:50, 1:20] <- 10 # Upregulated gene
counts <- matrix(rnbinom(length(means), mu=means, size=20), nrow=nrow(means), ncol=ncol(means))
nonzero <- rowSums(counts==0)==0
sf <- DESeq2::estimateSizeFactorsForMatrix(counts[nonzero,])
plot(means[51,], sf)
```

Precision also drops with this strategy, which becomes relevant for the median-based estimator when you're dealing with few genes.
Pooling avoids the decrease in precision by allowing more genes to be used when computing the median.

```{r}
means <- matrix(5, nrow=1000, ncol=50)
counts <- matrix(rnbinom(length(means), mu=means, size=20), nrow=nrow(means), ncol=ncol(means))
for (x in c(10, 20, 50, 100, 200, 500, 1000)) { 
    nonzero <- rowSums(counts[1:x,]==0)==0
    sf <- DESeq2::estimateSizeFactorsForMatrix(counts[1:x,][nonzero,])
    print(mad(log(sf/1)))
}
```

## Is accurate normalization really necessary?

Normalizing on library size is usually sufficient for purposes of cell type identification.
Separation of cell types is robust as the presence of differences in the expression profile is not affected by the scaling factor.
Indeed, that a cell type exhibits strong composition biases indicates that it has plenty of DE genes to distinguish it from other cell types.

However, the interpretation of these differences is sensitive to the choice of normalization strategy.
For example, depending on whether you use spike-in or non-DE normalization, you can flip the sign of the log-fold changes.
This could completely alter your determination of what the clusters actually represent.

## Explaining the zero-induced biases in existing methods

The explanation in the main text does not need to consider undefined ratios (i.e., due to a zero in the denominator).
For DESeq, there are none, as all ratios are defined after the workaround.
For edgeR, the number of undefined ratios that are removed is constant when the same reference cell is used for each comparison.
Thus, differences will be driven by zeroes in the numerator of the ratios.

# Summation and deconvolution with linear equations

## Justifying the pretense of a constant mean

The intuition is that when $U_i$ doesn't vary much, it doesn't have much effect on the variability of the ratio, and can basically be considered as fixed.
We don't need to worry about correlations between $V_{ik}$ and $U_{i}$ when considering the quality of the approximation.
See the explanation at www.stat.cmu.edu/~hseltman/files/ratio.pdf, where a second-order Taylor expansion yields:
$$
E(V/U) \approx \frac{E(V)}{E(U)} - \frac{cov(V, U)}{E(U)^2} + \frac{var(U)E(V)}{E(U)^3}
$$
Yes, there's a covariance in the error term, but this will drop as the variance of $U_i$ drops.

More precisely, we note that, in this setting, $U=(V+X)/N$ where $X$ is independent of $V$ and represents all the other cells.
As a result,
$$
E(V/U) \approx \frac{E(V)}{E(U)} - \frac{cov(V, V+X)}{E(U)^2N} + \frac{var(V+X)E(V)}{E(U)^3N^2}
$$
The error term then becomes (due to independence between $V$ and $X$):
$$
\begin{aligned}
\frac{-var(V)}{E(U)^2N} + \frac{var(V+X)E(V)}{E(U)^3N^2} 
&= \frac{1}{E(U)^3N^2} [ var(V+X)E(V) - N var(V) E(U) ] \\
&= \frac{1}{E(U)^3N^2} [ var(V)E(V) + var(X)E(V) - var(V)E(V) - var(V)E(X) ] \\
&= \frac{1}{E(U)^3N^2} [ var(X)E(V) - var(V)E(X) ]
\end{aligned}
$$
Further assume that all cells are represented by random variable $Y$.
Let's say that $V$ is made of a sum of $P$ cells, which also means that $X$ is made up of a sum of $N-P$ cells.
Thus,
$$
\begin{aligned}
E(V) &= P E(Y) \;, \\
var(V) &= P var(Y) \;, \\
E(X) &= (N-P) E(Y) \; \mbox{and} \\
var(X) &= (N-P) var(Y) \;.
\end{aligned}
$$
The error term can then be broken down into
$$
\frac{1}{E(U)^3N^2} [ (N-P)P E(Y)var(Y) - (N-P)P E(Y)var(Y) ] = 0
$$
So the approximation is actually pretty good. I'm not even sure we need the LLN.

## Justifying the median-based estimator

Another issue is with whether the expectation can be estimated with the median.
This should be the case for pooled counts, where the sum should approach normality (via CLT);
or, a NB distribution with increasing mean and decreasing dispersion such that the mean is close to the median.
This seems to be the case even at low means with the smallest pool size:

```{r}
ngenes <- 10000
true.means <- rgamma(ngenes, 2, 2)
dispersions <- 0.1
blah <- matrix(rnbinom(ngenes*20, mu=true.means, size=1/dispersions), nrow=ngenes)
hist(rowSums(blah)/true.means)
```

This is one of the motivations for filtering beforehand, to get rid of genes with small means for which this approximation is unlikely to hold
(this would result in a biased estimate of $E(R_{ik})$, which is likely to propagate to the size factors, as it won't cancel out exactly between equations).
Another way to interpret this is that low count, high dispersion genes will have highly variable $R_{ik}$.
This would decrease the precision _and_ accuracy of a median-based estimator.

I should also point out that taking the median across genes is permissible because the expected value of $R_{ik}$ doesn't actually depend on the gene (despite its notation).
Gene-specific factors cancel out, which means that the expectations are equal across genes, thus allowing us to (robustly) average across genes.

# Constructing the linear system by selecting cell pools

## Why use different pool sizes?

The different pool sizes probably help due to the inefficiency of the median estimator, such that even nested pools provide extra information.
To a smaller extent, the smaller pools mitigate errors from pooling together cells with very different size factors.
The larger pools also help for very low counts where the smaller pools might not be accurate with respect to the median-based estimator (or have too many zeroes).
Larger counts also result in more precise ratios, which obviously helps with reducing the error from the median estimate.
See standerr.Rout where the estimates with all pool sizes are less variable than just those with large or small pools.
(Incidentally, there doesn't seem to be a problem with using very large pools; so I got rid of the warning asking for pool sizes below half the number of cells.)
Also see my thoughts on how using multiple pools increases column rank and reduces imprecision due to unestimable coefficients.

The minimum pool size is chosen to reduce the number of zeros. 
With just 20 cells, it's possible to have >95% randomly distributed zeros and still get a non-zero pooled size factor (0.95^20 < 0.5)
In practice, the effective/tolerable proportion of zeroes is probably lower because some zeros are semi-systematic and you need multiple non-zero counts for the median to work.
Smaller cells will have more stochastic zeros anyway, but still; the equivalent failure point for DESeq would be 50%, which is clearly worse.

## Why use library size-sorted ring

The main reason for sorting by library size is actually different to that described.
Specifically, cells with small library size will have more discrete counts and low $t_j$.
This results in large, discrete $Z_{ij}$ values after library size adjustment.
To improve the precision of the median ratio estimate, we need to reduce discreteness, and this can only be done by adding together values of similar size.
This motivates the use of the library size-sorted ring to group cells with similar levels of discreteness together.

There is no point adding smaller, continuous $Z_{ij}$ from large cells, as the discreteness will effectively manifest as DE and skew the pooled size factor estimate.
In other words, convergence to a normal distribution or low-dispersion NB distribution would be compromised.
Indeed, randomly scattering these cells around the ring will reduce the accuracy of all cells, large and small.
It's not just a matter of the small cells having larger variance.
If it were, one would expect that random placement would actually improve the accuracy of small cells (as they get pooled with the more precise large cells).

## The residual variance has no meaning here

In theory, we should also weight on the number of cells in each pool, as size factors for larger pools are more variable with larger counts and should be downweighted.
One can imagine that each cell-specific size factor has some estimation variance, so the variance of the pool size factor would equal the sum of variances for its cells.
Weighting will give a more accurate estimate of the standard error, but for some reason, has little effect on the precision of the size factor estimates themselves. 
This is probably because the residuals are correlated across equations, due to the fact that the pooled size factors are computed from the same counts.
For example, if the residuals were perfectly correlated, the relative weighting of the equations wouldn't matter as the system is overdetermined and consistent.
Precision weights would also favour pools with few cells that are less likely to give accurate pool-based size factors due to zeroes and discreteness.

Note that least-squares doesn't require normality or independence in the response variables in this application.
It should still give correct results for non-normal distributions in the pool-based size factors, and for correlations between equations.
Solving the system just involves fiddling with linear equations to obtain an estimate for each cell-based size factor.
For linear combinations, the expected value of each estimate should not be affected by the distribution or correlations.
However, the standard error will be affected, which is probably why we shouldn't be estimating the standard error from this model.

# Obtaining sensible least-squares solutions

Sporadic negative size factors arise from low-quality cells with true size factors near zero.
These are most obviously handled by just filtering those low-quality cells out, e.g., if they do not express many genes.
If you see a large number of negative size factors, these are probably due to failure to filter out low-abundance genes.
This results in all cells getting median expression values of zero.

# Comments on filtering 

## Choosing an independent filter statistic

With an independent filter, the expected expression for a non-DE gene (conditional on its retention) is proportional to the size factor for each cell.
This ensures that the size factor estimates are not biased by the gene selection.
Indeed, we can see that the expected sum of counts for the retained non-DE genes would be directly proportional to the true size factor.

The library size-adjusted average count is a logical choice for filtering, given that the same adjustment (by $t_j$) is performed to obtain internal values for pooling.
However, as a filter statistic, this is not quite independent as it puts too much weight on counts from cells with small library sizes.
Non-DE genes that are stochastically higher in such cells would be given more weight and would be more likely to survive the filter.
This would potentially inflate the size factors for the smaller cells (equivalent to the problem encountered when picking genes with all non-zero counts).

The only way to ensure independence would be to use a count-based model, but this is very unattractive given the need to estimate the dispersion.
In any case, the library size-adjusted average is probably better than the direct average, which puts too much weight on counts from cells with large library sizes.
(We used the raw average in the manuscript because we didn't know any better at this point.)
Very small cells cause problems anyway and should have already been removed, which mitigates the problems with having too much weight on small cells.
In contrast, we can't really remove the large cells, so excess weight on them would be an issue.

The fact that the library size may not be equal to the size factor is not an issue.
For non-DE genes, the library-size adjusted expected count should be $s_j\mu/t_j$, which allows us to factorize out $\mu$.
This means that the ordering of non-DE genes should be preserved in the library size-adjusted average.
The only issue is whether the library size adjustment gives too much or too little weight to some cells, which we have already discussed above.

In practice, the choice of filtering strategy doesn't have a large impact, probably because the density of genes at the filter boundary is relatively low in RNA-seq data.
Stochastic changes in expression are also less likely to be a problem with large numbers of cells that stabilize the average.
We also use a robust estimator of the size factor to protect against DE.

## Filtering to protect the median

As discussed before, low counts are problematic for median-based normalization.
This is because the count:mean ratios are less precise and the median is not accurate estimator of the mean.
Consider the following simulation, where we compute the median for 10000 low-abundance genes, and calculate its expectation and variance.

```{r}
set.seed(100)
ngenes <- 10000
niter <- 100
vals <- list()
for (it in seq_len(20)) {
    lambda <- it/4
    y <- matrix(rpois(ngenes*niter, lambda=lambda), ncol=ngenes)
    rat <- y/rowMeans(y)
    meds <- apply(rat, 1, median)
    vals[[it]] <- c(lambda, mean(meds), sd(meds))
}
vals <- do.call(rbind, vals)
```

You can see that at counts below 2, the estimation error from using the median is pretty bad (exceeding 20% from the expected value of unity).
It's at a similar point that the variance of the estimator also increases, though the bias is the major contributor here,
Obviously you can expect that this will get even worse for overdispersed data where both the variance and bias are likely to increase.

```{r, fig.width=10, fig.height=6}
par(mfrow=c(1,2))
plot(vals[,1], vals[,2], xlab="Lambda", ylab="Expected median ratio")
plot(vals[,1], vals[,3], xlab="Lambda", ylab="Standard deviation")
```

In the deconvolution method, you can consider the threshold multiplied by the minimum pool size (20 cells by default) as the minimum pooled count.
This gives us pooled counts of around `20*1` for read data, which should be large enough to avoid the above problems for NB dispersions below 0.5.
For UMI data, we get pooled counts of `20*0.1` -- however, the variability is near-Poisson anyway, so variability and bias in the ratio should be okay.
Any biases are probably tolerable if the data set is dominated by genes in the the high-abundance peak.
(They may also cancel out to some extent, given that the bias fluctuates in the plot above).

## Filtering within and between clusters

It makes sense to perform filtering within each cluster rather than doing it globally.
The former ensures that deconvolution is operating on appropriately high-abundance genes within each cluster.
Otherwise, the choice of genes would be skewed by larger clusters, and such genes may not be expressed to an appropriate level in other clusters.
Thus, `computeSumFactors` will filter by the chosen minimum mean within each specified cluster.

When normalizing between clusters, we only use the genes where the _grand mean of the means_ across the two clusters is greater than the specified threshold.
This is a better choice than the mean across all cells, or the mean across all cells of the two clusters, as the grand mean is insensitive to the cluster sizes.
Thus, we are less likely to enrich for DE genes that are upregulated in the larger cluster.
(In contrast, we're more likely to select balanced DE.)

To further improve selection of balanced DE, the grand mean is also computed in a manner that is agnostic to the mean total counts of each cluster.
Specifically, we compute the average proportion for each gene and then multiply this by the average total count across the two clusters.
This avoids favouring DE genes that are only upregulated in the cluster that has larger total counts.

```{r}
mu1 <- rep(c(10, 50, 100), each=1000)
mu2 <- rep(c(10, 5, 1), each=1000)Â # 10x lower counts, due to fewer/smaller cells.
mode <- rep(c("Down", "Same", "Up"), each=1000)
a1 <- rnbinom(10000, mu=mu1, size=10)
a2 <- rnbinom(10000, mu=mu2, size=10)

# Raw count filtering (keeping the top 10%)
nkeep <- length(mu1)/10
keep <- rank(-(a1 + a2)) <= nkeep
table(mode[keep])

# Filtering after adjusting for library size - more balanced.
keep <- rank(-scater::calcAverage(cbind(a1, a2))) <= nkeep
table(mode[keep])
```

We also filter before we pre-cluster (see below) to ensure that we have the same set of genes being used in `quickCluster` and `computeSumFactors`.
This ensures that we focus on separating cells by the DE genes (or lack thereof) in the set of genes to be used for normalization.
There's no need to worry about DE genes in the low-abundance genes, because we've filtered them out already.
Obviously, filtering out low-abundance genes will discard some reoslution of biological structure.
This is tolerable as the point of pre-clustering is to avoid DE genes within each cluster, not to identify genuine subpopulations.

# Clustering to weaken the non-DE assumption

## When is the assumption violated?

If every gene is DE in a subset of cells, the average expression will include some contribution from DE for each gene.
This means that the median ratio of the pool sum to the average pseudo-cell would no longer solely represent bias.
Instead, the ratio would also include some arbitrary DE between cells in and out of the pool.
The only situation in which it would be okay is if the amount of DE is the same for each gene, such that the relative size factors are unchanged.
However, this seems like it would be unlikely.

In fact, the effective failure point with respect to the DE proportion is much lower than 50%.
Size factors will become increasingly inaccurate as we introduce a greater imbalance in the proportion of up/down-regulated DE genes.
This is because the median gets shifted away from the true non-DE mean in the presence of skews in the ratio distribution.
The simulations in `moresim` demonstrate this effectively, though the deconvolution method is still more accurate than library size normalization in most cases.
This is part of its selling point; while clustering is still required, it is robust to imprecisions in the clustering that leave some DE genes behind.

## Explaining the rescaling

The rescaling is justified by considering that the within-cluster normalization removes biases between each cell and the cluster-specific pseudo cell.
The $\tau$ represents the scaling between pseudo-cells of different clusters, to remove systematic biases between clusters.
Thus, by scaling all size factors by $\tau$, you eliminate differences between the cluster-specific pseudo cells.
This effectively means that all cells in all clusters are, now, scaled to the same pseudo cell.

In more detail, the cluster-based re-scaling technically refers to the normalization factors with the library size-adjusted expression values.
However, you would end up multiplying by the library size of each cell anyway to obtain the size factors.
There is no harm in multiplying by the library size first, and then scaling by $\tau$, given that the former has no effect on the calculation on the latter.

Or in other words; the aim of the normalization is to remove systematic differences in coverage.
Conceptually, it doesn't matter whether you do this all at once, or remove differences within clusters first followed by removing differences between clusters.
The end result is still the same, i.e., differences are removed.
By comparison, pre-processing methods that use clustering to preserve differences in some manner (e.g., imputation) are inherently more suspect,
due to circularity when used with downstream clustering.

## Effects of clustering

We can evaluate the sensitivity of the results on the clustering, simply by requesting more or less refined clusters.
Using the brain dataset from the _simpleSingleCell_ workflow as an example, we see that the algorithm is mostly robust to the number of clusters.
Correlations between size factor estimates with different numbers of clusters are clearly higher than those with the library size.
This is consistent with the robustness of the method to DE within each set of cells used for deconvolution.

```{r}
library(scran)
sce <- readRDS("brain_data.rds")

# Default.
clust1 <- quickCluster(sce, min.mean=0.1, method="igraph")
table(clust1)
sizefacs1 <- computeSumFactors(sce, clusters=clust1, min.mean=0.1, sf.out=TRUE)
plot(log2(sizefacs1), log2(colSums(counts(sce))))

# More clusters by reducing the number of neighbors.
clust2 <- quickCluster(sce, min.mean=0.1, k=5, method="igraph")
table(clust2)
sizefacs2 <- computeSumFactors(sce, clusters=clust2, min.mean=0.1, sf.out=TRUE)
plot(log2(sizefacs1), log2(sizefacs2))

# Fewer clusters by increasing the number of neighbours.
clust3 <- quickCluster(sce, min.mean=0.1, k=20, method="igraph")
table(clust3)
sizefacs3 <- computeSumFactors(sce, clusters=clust3, min.mean=0.1, sf.out=TRUE)
plot(log2(sizefacs1), log2(sizefacs3))
```

Our reasoning suggests that overclustering would be ideal for minimizing the DE between cells to be pooled.
This is true enough, provided that sufficient cells are still present in each cluster for pooling.
A more subtle effect is there will be more all-zero and low-abundance genes in each cluster, due the structure of the semi-systematic zeroes.
As such, it is important to re-filter _within_ each cluster to ensure that only informative genes are used during pooled size factor estimation.
An even more subtle effect is that the cells are no longer strictly independent; cells that are stochastically similar are more likely to be placed in the same cluster.
This weakens the convergence to normality via the CLT, though this would require very strong correlations between the majority of non-DE genes to have any impact.

## Choosing the reference cluster

It is tempting to think that we could use a nearest-neighbour strategy for rescaling.
Namely, if clusters $A$, $B$, $C$ and $D$ lie on a straight line, it would make sense that we would rescale the closest clusters first, e.g., $A$ to $B$.
We could then rescale $B$ to $C$, and then $C$ to $D$, taking advantage of the greater accuracy of median-based normalization when dealing with similar populations.

The problem is that, when we rescale $B$ to $C$, we also have to re-rescale $A$ using the $B\to C$ scaling factor.
This results in multiplicative errors that are almost as bad as rescaling $A$ to $C$ directly.
To illustrate, let's assume a normal model for the log-fold changes between clusters.
We can examine the error in using the median-based estimator as a function of the proportion of DE genes in one direction.

```{r}
p.de <- seq(0, 0.5, length.out=50)
p.nde <- 1 - p.de
error <- qnorm(p.nde * 0.5)
plot(p.de, error)
```

We observe that it is effectively linear, meaning that the error at 20% DE genes is twice that at 10% DE genes.
Consider a situation where $A$ and $B$ and $B$ and $C$ were separated by a different set of 10% DEGs.
The error from double rescaling of $A\to B$ and $B\to C$ would be equivalent to the error from just rescaling from $A\to C$ directly.

Of course, this is already the most optimistic scenario.
Consider instead a situation where $A$, $B$ and $C$ are separated by the same set of 10% DE genes, of varying levels of expression in each cluster.
Double rescaling would result in an effective 20% error from $A$ to $C$, even though a direct rescaling would only have 10%.

## Quick clustering with binarized data

An alternative to using `quickCluster` is to cluster on whether a gene is expressed or not.
This is technically invariant to scaling, avoiding circularity between clustering and normalization.
However, the clusters are more affected by bias than if rank correlations had been used, as the number of expressed features is tightly correlated to the library size.
This means that you run the risk of clustering by library size rather than by actual differences in the expression profiles.

```{r}
# Constructing an expression matrix.
set.seed(100)
ngenes <- 1000
prof1 <- runif(ngenes)
prof2 <- runif(ngenes)
true.means <- cbind(outer(prof1, rep(c(0.5, 2), each=50)),
                    outer(prof2, rep(c(0.5, 2), each=50)))
counts <- rpois(length(true.means), lambda=true.means)
dim(counts) <- dim(true.means)

# Computing distances with these two methods.
by.cor <- sqrt(0.5*(1-cor(counts, method="spearman")))
by.bin <- as.matrix(dist(t(counts)>0))

# Figuring out which ones are good and bad.
cell.ids <- paste0(rep(c("A", "B"), each=100), 
                   rep(c(".s", ".l", ".s", ".l"), each=50))
pair.ids <- outer(cell.ids, cell.ids, FUN=paste, sep="-")

par(mfrow=c(1,2))
keep <- lower.tri(by.cor)
cor.box <- split(by.cor[keep], pair.ids[keep])
boxplot(cor.box, col=ifelse(grepl("A.*A", names(cor.box)) | grepl("B.*B", names(cor.box)), "grey80", "red"))
bin.box <- split(by.bin[keep], pair.ids[keep])
boxplot(bin.box, col=ifelse(grepl("A.*A", names(bin.box)) | grepl("B.*B", names(bin.box)), "grey80", "red"))
```

The above simulation demonstrates this point, where small cells are more likely to cluster together with binarized data, irrespective of the population of origin.
In contrast, the rank correlations are more appropriate as distance measures.
Distances increase with small cells where the data are noisier, this is inevitable regardless of what method you use.

# Supplementary details

## Performance of DESeq normalization after addition of a pseudo-count

The flipped away-from-unity bias with library size-adjusted pseudo counts is an interesting effect.
This is probably caused by the lack of precision of the median-based estimator, especially when it is occurring at a count that was originally zero.
For simplicity, assume that the library size is the true size factor, so that the ratio of each gene's mean count to the average reference would be a perfect estimate.
(This is true even with addition of a pseudo count, due to the library size adjustment.)
However, the median ratio for each cell will be lower than the expected ratio if it is computed from a count that was originally zero.
This results in underestimation of the size factor for small cells, and concomitant overestimation for the large cells after centering.
We get a smooth line rather than a sharp jump from zero to 1, as the ratio will steadily increase due to the prior count. 

Obviously, this is a moot point as it depends on the library size being an accurate estimate of the size factor.
This won't be true with DE, which is the entire point of using alternative normalization procedures in the first place.

## Resolving linear dependencies in the constructed system

For the larger example, replace $x$ with $a$ for the first column, $b$ for the second, and so on, until the 20th column, where we add the negative sum of $a + b +...$.
If we formulate the same RSS equation and take the partial derivative, we find that the minimum for each expression is at a negative scaled sum of all other parameters.
(Plus the difference between the direct estimates and the true values, which has an expected value of zero anyway.)
This only has a solution when all parameters are equal to zero, due to the non-unity scaling; so the logic above still applies.
Note that, because we split it into 19 parameters (+1 for the negative sum of everyone), each $J$ set will be of size 10.
Similarly, the 1/4 at the start will become 1/20 as you'll add up 20 $a^2$ terms.
Thus, the value of $a$ will converge to 0 with increasing size of $J$, as the sum will be divided by the number of cells in $J$.

I also realized that, in practice, the direct estimator will not be entirely unbiased for UMI data as there's too many zeros.
In such cases, the median $\tilde\theta_j$ will (almost always) be equal to zero if there's more than 50% zeroes.
Fortunately, $x$ will still approach zero if the mean of $theta_j$ in $J_1$ is equal to the mean of $theta_j$ in $J_2$.
$J_1$ should always be of equal size to $J_2$, as for every addition of $x$ you'll need a subtraction of $x$ somewhere else to maintain the infinite solutions. 
And, given that the elements of $J_1$ and $J_2$ are alternately distributed around the ring, there shouldn't be any systematic differences in the mean size factors between sets.

The same logic applies for any pool size and number of cells that are not coprime.
The number of variables that need to rely on the above expectation result is equal to the GCD minus 1.
More generally, this is equal to the number of unestimable coefficients, i.e., the difference between the number of cells and the column rank of the matrix.
This motivates the use of multiple pool sizes, which should reduce the number of unestimable coefficients (and increase each J, thus improving precision).
For example, with a single pool size of 20 you'll have 19 unestimable coefficients, whereas with pool sizes from 20 to 100 (going up by 5), it drops to 4.
Admittedly, it is rather odd that a system with three cells and a pool size of 2 is solvable, while a system with four cells is not.

## Implementation details of the clustering approach

Someone asked me if we should mean-center the genes before computing correlations (e.g., like PCA).
This is probably unwise.
Two cells that are similar to the mean expression profile will have uncorrelated residuals and would appear to be unrelated. 
The correct correlation should be near unity as their expression values would match up perfectly. 
Use of uncentered counts is also easier to interpret and ensures insensitivity to normalization.

## Comparing normalization accuracy on real data

The low MAD for DESeq is probably because of its inaccuracy, where the size factors are constrained by discreteness.
At higher counts, DESeq and deconvolution approach the same precision, which makes sense as they are computed in basically the same manner.
Library size normalization is most efficient at using information but obviously is only applicable when there is no DE.
We can increase precision for deconvolution by using more fine-grained sizes (set sizes=2:10*10 in standerr.R), but this comes at the cost of computational work.
